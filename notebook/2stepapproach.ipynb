{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the padding and resize function with gray padding\n",
    "def padding_and_resize(img, target_size=224, fill=(128, 128, 128)):\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        if w > h:\n",
    "            delta = w - h\n",
    "            padding = (0, delta // 2, 0, delta - delta // 2)\n",
    "        else:\n",
    "            delta = h - w\n",
    "            padding = (delta // 2, 0, delta - delta // 2, 0)\n",
    "        img = ImageOps.expand(img, padding, fill=fill)\n",
    "    img = img.resize((target_size, target_size), Image.Resampling.BILINEAR)\n",
    "    return img\n",
    "\n",
    "# Apply CLAHE to enhance contrast\n",
    "def apply_clahe(img):\n",
    "    img_np = np.array(img.convert('L'))  # Convert to grayscale\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_np)\n",
    "    img = Image.fromarray(cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001, path='/kaggle/working/stage1_best.pth'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(f\"Model saved with val_loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: apply_clahe(img)),\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224, fill=(128, 128, 128))),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "general_augment_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: apply_clahe(img)),\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224, fill=(128, 128, 128))),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dr_augment_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: apply_clahe(img)),\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224, fill=(128, 128, 128))),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BinaryDataset\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, dataset, base_transform=None, general_augment_transform=None, dr_augment_transform=None, is_train=False):\n",
    "        self.dataset = dataset\n",
    "        self.base_transform = base_transform\n",
    "        self.general_augment_transform = general_augment_transform\n",
    "        self.dr_augment_transform = dr_augment_transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        if self.is_train:\n",
    "            if label != 0:\n",
    "                img = self.dr_augment_transform(img)\n",
    "            else:\n",
    "                img = self.general_augment_transform(img)\n",
    "        else:\n",
    "            img = self.base_transform(img)\n",
    "        binary_label = 0 if label == 0 else 1\n",
    "        return img, binary_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "data_dir = \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS\"\n",
    "full_train_dataset_raw = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=None)\n",
    "val_dataset_raw = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=None)\n",
    "\n",
    "# Create datasets with is_train flag\n",
    "train_dataset = BinaryDataset(full_train_dataset_raw, base_transform=base_transform, general_augment_transform=general_augment_transform, dr_augment_transform=dr_augment_transform, is_train=True)\n",
    "val_dataset = BinaryDataset(val_dataset_raw, base_transform=base_transform, general_augment_transform=general_augment_transform, dr_augment_transform=dr_augment_transform, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute class weights for sampler and loss\n",
    "train_labels = [0 if l == 0 else 1 for l in full_train_dataset_raw.targets]\n",
    "class_counts = Counter(train_labels)\n",
    "class_weights = [1.0 / class_counts[i] for i in range(2)]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum() * 2  # Normalize\n",
    "sample_weights = [class_weights[label].item() for label in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader_stage1 = DataLoader(train_dataset, batch_size=64, sampler=sampler, pin_memory=True)\n",
    "val_loader_stage1 = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_stage1 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# Unfreeze last block of features\n",
    "for param in model_stage1.features[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "num_ftrs = model_stage1.classifier[3].in_features\n",
    "model_stage1.classifier[3] = nn.Linear(num_ftrs, 2)\n",
    "model_stage1 = model_stage1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion_stage1 = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_stage1 = optim.Adam(model_stage1.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_stage1, mode='min', factor=0.1, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:28:50.950737Z",
     "iopub.status.busy": "2025-09-16T17:28:50.950479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 159MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage1 Epoch 1/20: 100%|██████████| 17/17 [04:45<00:00, 16.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage1 Epoch 1: Train Loss 0.4879, Train Acc 0.5607, Val Loss 0.7999, Val Acc 0.3776, Macro F1 0.3754\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.21      0.34       398\n",
      "           1       0.27      0.89      0.41       129\n",
      "\n",
      "    accuracy                           0.38       527\n",
      "   macro avg       0.56      0.55      0.38       527\n",
      "weighted avg       0.71      0.38      0.36       527\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with val_loss: 0.7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage1 Epoch 2/20: 100%|██████████| 17/17 [04:38<00:00, 16.40s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "\n",
    "# Training loop\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs, val_macro_f1s = [], [], []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(20):  # Increased epochs due to lower learning rate\n",
    "    # Training Phase\n",
    "    model_stage1.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader_stage1, desc=f\"Stage1 Epoch {epoch+1}/20\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_stage1.zero_grad()\n",
    "        outputs = model_stage1(inputs)\n",
    "        loss = criterion_stage1(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_stage1.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader_stage1.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "\n",
    "    # Validation Phase\n",
    "    model_stage1.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_stage1:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_stage1(inputs)\n",
    "            loss = criterion_stage1(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader_stage1.dataset)\n",
    "    val_acc = correct / total\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_macro_f1s.append(macro_f1)\n",
    "    \n",
    "    print(f\"Stage1 Epoch {epoch+1}: Train Loss {epoch_loss:.4f}, Train Acc {epoch_acc:.4f}, Val Loss {val_loss:.4f}, Val Acc {val_acc:.4f}, Macro F1 {macro_f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No-DR', 'DR'], yticklabels=['No-DR', 'DR'])\n",
    "    plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Early stopping and scheduler\n",
    "    early_stopping(val_loss, model_stage1)\n",
    "    scheduler.step(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Load best model\n",
    "model_stage1.load_state_dict(torch.load(\"/kaggle/working/stage1_best.pth\"))\n",
    "print(\"Stage 1 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Padding & resize function with gray padding\n",
    "def padding_and_resize(img, target_size=224, fill=(128, 128, 128)):\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        if w > h:\n",
    "            delta = w - h\n",
    "            padding = (0, delta // 2, 0, delta - delta // 2)\n",
    "        else:\n",
    "            delta = h - w\n",
    "            padding = (delta // 2, 0, delta - delta // 2, 0)\n",
    "        img = ImageOps.expand(img, padding, fill=fill)\n",
    "    img = img.resize((target_size, target_size), Image.Resampling.BILINEAR)\n",
    "    return img\n",
    "\n",
    "# Apply CLAHE for contrast enhancement\n",
    "def apply_clahe(img):\n",
    "    img_np = np.array(img.convert('L'))  # Convert to grayscale\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_clahe = clahe.apply(img_np)\n",
    "    img = Image.fromarray(cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB))\n",
    "    return img\n",
    "\n",
    "# Transform for inference (matches training/validation base_transform)\n",
    "stage1_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: apply_clahe(img)),\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224, fill=(128, 128, 128))),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Stage 1 model\n",
    "model_stage1 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model_stage1.classifier[3].in_features\n",
    "model_stage1.classifier[3] = torch.nn.Linear(num_ftrs, 2)\n",
    "model_stage1.load_state_dict(torch.load(\"/kaggle/working/stage1_best.pth\"))\n",
    "model_stage1 = model_stage1.to(device)\n",
    "model_stage1.eval()\n",
    "\n",
    "# Inference function\n",
    "def predict_stage1(img_paths):\n",
    "    results = []\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            input_tensor = stage1_transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model_stage1(input_tensor)\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "                pred_class = torch.argmax(probs, dim=1).item()\n",
    "                confidence = probs[0, pred_class].item()\n",
    "            \n",
    "            results.append((img_path, pred_class, confidence))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            results.append((img_path, None, None))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "image_paths = [\n",
    "    \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/train/3/23787_left.jpeg\",\n",
    "    \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test/0/10050_left.jpeg\",\n",
    "    \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test/0/10451_left.jpeg\"\n",
    "    # Add more image paths here for batch inference,\n",
    "]\n",
    "results = predict_stage1(image_paths)\n",
    "for img_path, pred_class, conf in results:\n",
    "    if pred_class is not None:\n",
    "        print(f\"Image: {img_path}, Predicted Class: {pred_class} (0: No-DR, 1: DR), Confidence: {conf:.4f}\")\n",
    "    else:\n",
    "        print(f\"Image: {img_path}, Failed to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create output dir\n",
    "out_dir = \"/kaggle/working/metrics_stage11\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Plot Loss & Accuracy curves\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/loss_acc_curves.png\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Macro F1 curve\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(val_macro_f1s, marker='o', label='Val Macro F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Macro F1')\n",
    "plt.title('Validation Macro F1 Curve')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{out_dir}/macro_f1_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Per-class metrics\n",
    "# -------------------------------\n",
    "# Run final evaluation on validation set\n",
    "model_stage1.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_stage1:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_stage1(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report (text)\n",
    "report = classification_report(all_labels, all_preds, digits=4)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "with open(f\"{out_dir}/classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Arrays of precision, recall, f1 per class\n",
    "prec, rec, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "np.savez(f\"{out_dir}/per_class_metrics.npz\",\n",
    "         precision=prec, recall=rec, f1=f1, support=support)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Confusion Matrix\n",
    "# -------------------------------\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_names = [str(i) for i in range(len(cm))]  # adjust if you have custom labels\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(f\"{out_dir}/confusion_matrix.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, classification_report\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset wrapper to filter classes and remap labels\n",
    "class FilteredImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, exclude_classes=[0], label_map=None):\n",
    "        self.dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        self.indices = [idx for idx, (_, label) in enumerate(self.dataset) if label not in exclude_classes]\n",
    "        if label_map is None:\n",
    "            # Automatically remap labels to 0-(num_classes-1)\n",
    "            unique_labels = sorted(set(self.dataset.targets[idx] for idx in self.indices))\n",
    "            self.label_map = {old: new for new, old in enumerate(unique_labels)}\n",
    "        else:\n",
    "            self.label_map = label_map\n",
    "        self.num_classes = len(self.label_map)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[self.indices[index]]\n",
    "        new_label = self.label_map.get(label, -1)\n",
    "        if new_label == -1:\n",
    "            raise ValueError(f\"Unexpected label {label}\")\n",
    "        return img, new_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights for imbalanced dataset\n",
    "def compute_class_weights(dataset):\n",
    "    labels = [label for _, label in dataset]\n",
    "    class_counts = np.bincount(labels)\n",
    "    weights = 1. / class_counts\n",
    "    weights = weights / weights.sum() * len(class_counts)  # Normalize\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform function\n",
    "def padding_and_resize(img, target_size=224, fill=(128,128,128)):\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        if w > h:\n",
    "            delta = w - h\n",
    "            padding = (0, delta // 2, 0, delta - delta // 2)\n",
    "        else:\n",
    "            delta = h - w\n",
    "            padding = (delta // 2, 0, delta - delta // 2, 0)\n",
    "        img = ImageOps.expand(img, padding, fill=fill)\n",
    "    img = img.resize((target_size, target_size), Image.Resampling.BILINEAR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def load_data(train_dir, val_dir, batch_size=32):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Lambda(lambda img: padding_and_resize(img)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Lambda(lambda img: padding_and_resize(img)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Assuming classes are folders '0', '1', '2', '3', '4' and we exclude '0'\n",
    "    # Labels 1->0, 2->1, 3->2, 4->3\n",
    "    label_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "    train_dataset = FilteredImageFolder(train_dir, transform=transform_train, exclude_classes=[0], label_map=label_map)\n",
    "    val_dataset = FilteredImageFolder(val_dir, transform=transform_val, exclude_classes=[0], label_map=label_map)\n",
    "\n",
    "    class_weights = compute_class_weights(train_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, class_weights, train_dataset.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "def get_model(num_classes):\n",
    "    model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
    "    # Modify the classifier for 4 classes\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, class_weights, num_epochs=50, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curves\n",
    "def plot_curves(train_losses, val_losses, train_accs, val_accs):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'bo-', label='Training acc')\n",
    "    plt.plot(epochs, val_accs, 'ro-', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Class 1 (0)', 'Class 2 (1)', 'Class 3 (2)', 'Class 4 (3)']))\n",
    "    print('F1 Score:', f1_score(all_labels, all_preds, average='weighted'))\n",
    "    print('Recall:', recall_score(all_labels, all_preds, average='weighted'))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def infer(model, image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda img: padding_and_resize(img)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor.to(device))\n",
    "        _, predicted = output.max(1)\n",
    "    # Map back to original labels: 0->1, 1->2, 2->3, 3->4\n",
    "    original_label = predicted.item() + 1\n",
    "    return original_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume your data is in 'data/train' and 'data/val' with subfolders '0','1','2','3','4'\n",
    "    train_dir = '/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/train'\n",
    "    val_dir = '/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test'\n",
    "    batch_size = 64\n",
    "\n",
    "    train_loader, val_loader, class_weights, num_classes = load_data(train_dir, val_dir, batch_size)\n",
    "    model = get_model(num_classes)\n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, class_weights)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'mobilenetv3_classes1-4.pth')\n",
    "\n",
    "    # Plot\n",
    "    plot_curves(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(model, val_loader)\n",
    "\n",
    "    # Example inference\n",
    "    predicted_class = infer(model, '/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test/3/34442_right.jpeg')\n",
    "    print(f'Predicted original class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_and_save_curves(train_losses, val_losses, train_accs, val_accs, save_path=\"training_curves.png\"):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'bo-', label='Training acc')\n",
    "    plt.plot(epochs, val_accs, 'ro-', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save instead of show\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Training curves saved to {save_path}\")\n",
    "\n",
    "plot_and_save_curves(train_losses, val_losses, train_accs, val_accs, save_path=\"/kaggle/working/train_val_curves.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save_cm(model, val_loader, cm_path=\"confusion_matrix.png\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification metrics\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Class 1 (0)', 'Class 2 (1)', 'Class 3 (2)', 'Class 4 (3)']))\n",
    "    print('F1 Score:', f1_score(all_labels, all_preds, average='weighted'))\n",
    "    print('Recall:', recall_score(all_labels, all_preds, average='weighted'))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(set(all_labels)))\n",
    "    plt.xticks(tick_marks, ['1','2','3','4'])\n",
    "    plt.yticks(tick_marks, ['1','2','3','4'])\n",
    "\n",
    "    # Add text\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to {cm_path}\")\n",
    "evaluate_and_save_cm(model, val_loader, cm_path=\"/kaggle/working/cm_stage2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Only inference\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# -------------------- Device --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------- Padding + Resize --------------------\n",
    "def padding_and_resize(img, target_size=224, fill=(0, 0, 0)):\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        if w > h:\n",
    "            delta = w - h\n",
    "            padding = (0, delta // 2, 0, delta - delta // 2)\n",
    "        else:\n",
    "            delta = h - w\n",
    "            padding = (delta // 2, 0, delta - delta // 2, 0)\n",
    "        img = ImageOps.expand(img, padding, fill=fill)\n",
    "    img = img.resize((target_size, target_size), Image.Resampling.BILINEAR)\n",
    "    return img\n",
    "\n",
    "# -------------------- Common Transform --------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# -------------------- Stage 1 Model (Binary: No-DR vs DR) --------------------\n",
    "model_stage1 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "num_ftrs1 = model_stage1.classifier[3].in_features\n",
    "model_stage1.classifier[3] = torch.nn.Linear(num_ftrs1, 2)\n",
    "model_stage1.load_state_dict(torch.load(\"/kaggle/input/firstfirst/pytorch/default/1/stage1_best.pth\", map_location=device))\n",
    "model_stage1 = model_stage1.to(device)\n",
    "model_stage1.eval()\n",
    "\n",
    "# -------------------- Stage 2 Model (DR Severity: 1–4) --------------------\n",
    "model_stage2 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "num_ftrs2 = model_stage2.classifier[3].in_features\n",
    "model_stage2.classifier[3] = torch.nn.Linear(num_ftrs2, 4)  # 4 classes\n",
    "model_stage2.load_state_dict(torch.load(\"/kaggle/working/mobilenetv3_classes1-4.pth\", map_location=device))\n",
    "model_stage2 = model_stage2.to(device)\n",
    "model_stage2.eval()\n",
    "\n",
    "# -------------------- Combined Inference --------------------\n",
    "def predict_overall(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # ----- Stage 1 -----\n",
    "        with torch.no_grad():\n",
    "            out1 = model_stage1(input_tensor)\n",
    "            probs1 = torch.softmax(out1, dim=1)\n",
    "            pred1 = torch.argmax(probs1, dim=1).item()\n",
    "            conf1 = probs1[0, pred1].item()\n",
    "\n",
    "        if pred1 == 0:  # No DR\n",
    "            return {\"image\": image_path, \"stage1_class\": \"No DR\", \"stage1_conf\": conf1,\n",
    "                    \"final_prediction\": \"No DR\"}\n",
    "        else:  # DR present → Stage 2\n",
    "            with torch.no_grad():\n",
    "                out2 = model_stage2(input_tensor)\n",
    "                probs2 = torch.softmax(out2, dim=1)\n",
    "                pred2 = torch.argmax(probs2, dim=1).item()\n",
    "                conf2 = probs2[0, pred2].item()\n",
    "\n",
    "            # Map stage2 prediction: 0->1, 1->2, 2->3, 3->4\n",
    "            severity = pred2 + 1\n",
    "            return {\"image\": image_path,\n",
    "                    \"stage1_class\": \"DR\", \"stage1_conf\": conf1,\n",
    "                    \"stage2_class\": severity, \"stage2_conf\": conf2,\n",
    "                    \"final_prediction\": f\"DR Severity {severity}\"}\n",
    "    except Exception as e:\n",
    "        return {\"image\": image_path, \"error\": str(e)}\n",
    "\n",
    "# -------------------- Example Usage --------------------\n",
    "image_paths = [\n",
    "    \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test/0/10701_left.jpeg\",\n",
    "]\n",
    "\n",
    "for path in image_paths:\n",
    "    result = predict_overall(path)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inference with attention map\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- Device --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------- Padding + Resize --------------------\n",
    "def padding_and_resize(img, target_size=224, fill=(0, 0, 0)):\n",
    "    w, h = img.size\n",
    "    if w != h:\n",
    "        if w > h:\n",
    "            delta = w - h\n",
    "            padding = (0, delta // 2, 0, delta - delta // 2)\n",
    "        else:\n",
    "            delta = h - w\n",
    "            padding = (delta // 2, 0, delta - delta // 2, 0)\n",
    "        img = ImageOps.expand(img, padding, fill=fill)\n",
    "    img = img.resize((target_size, target_size), Image.Resampling.BILINEAR)\n",
    "    return img\n",
    "\n",
    "# -------------------- Common Transform --------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: padding_and_resize(img, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# -------------------- Stage 1 Model (Binary: No-DR vs DR) --------------------\n",
    "model_stage1 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "num_ftrs1 = model_stage1.classifier[3].in_features\n",
    "model_stage1.classifier[3] = nn.Linear(num_ftrs1, 2)\n",
    "model_stage1.load_state_dict(torch.load(\"/kaggle/input/firstfirst/pytorch/default/1/stage1_best.pth\", map_location=device))\n",
    "model_stage1 = model_stage1.to(device)\n",
    "model_stage1.eval()\n",
    "\n",
    "# -------------------- Stage 2 Model (DR Severity: 1–4) --------------------\n",
    "model_stage2 = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "num_ftrs2 = model_stage2.classifier[3].in_features\n",
    "model_stage2.classifier[3] = nn.Linear(num_ftrs2, 4)  # 4 classes\n",
    "model_stage2.load_state_dict(torch.load(\"/kaggle/working/mobilenetv3_classes1-4.pth\", map_location=device))\n",
    "model_stage2 = model_stage2.to(device)\n",
    "model_stage2.eval()\n",
    "\n",
    "# -------------------- Grad-CAM --------------------\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss = output[:, class_idx]\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "        activations = self.activations.squeeze(0)\n",
    "\n",
    "        for i in range(len(pooled_gradients)):\n",
    "            activations[i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        heatmap = activations.mean(dim=0).cpu().numpy()\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= heatmap.max() + 1e-8\n",
    "        return heatmap\n",
    "\n",
    "# -------------------- Heatmap Overlay --------------------\n",
    "def overlay_heatmap(img_path, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET, save_path=None):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "\n",
    "    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
    "\n",
    "    if save_path:\n",
    "        plt.imsave(save_path, overlay)\n",
    "    return overlay\n",
    "\n",
    "# -------------------- Combined Inference + Grad-CAM --------------------\n",
    "def predict_with_gradcam(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # ----- Stage 1 -----\n",
    "    out1 = model_stage1(input_tensor)\n",
    "    probs1 = torch.softmax(out1, dim=1)\n",
    "    pred1 = probs1.argmax(dim=1).item()\n",
    "    conf1 = probs1[0, pred1].item()\n",
    "\n",
    "    # Grad-CAM for Stage 1\n",
    "    gradcam1 = GradCAM(model_stage1, model_stage1.features[-1])\n",
    "    heatmap1 = gradcam1.generate(input_tensor, class_idx=pred1)\n",
    "    overlay1 = overlay_heatmap(image_path, heatmap1, save_path=\"/kaggle/working/stage1_gradcam.png\")\n",
    "\n",
    "    if pred1 == 0:  # No DR\n",
    "        return {\"image\": image_path,\n",
    "                \"stage1_prediction\": \"No DR\",\n",
    "                \"stage1_conf\": conf1,\n",
    "                \"stage1_heatmap\": overlay1,\n",
    "                \"final_prediction\": \"No DR\"}\n",
    "\n",
    "    # ----- Stage 2 -----\n",
    "    out2 = model_stage2(input_tensor)\n",
    "    probs2 = torch.softmax(out2, dim=1)\n",
    "    pred2 = probs2.argmax(dim=1).item()\n",
    "    conf2 = probs2[0, pred2].item()\n",
    "\n",
    "    severity = pred2 + 1  # map 0→1, 1→2, 2→3, 3→4\n",
    "\n",
    "    # Grad-CAM for Stage 2\n",
    "    gradcam2 = GradCAM(model_stage2, model_stage2.features[-1])\n",
    "    heatmap2 = gradcam2.generate(input_tensor, class_idx=pred2)\n",
    "    overlay2 = overlay_heatmap(image_path, heatmap2, save_path=\"stage2_gradcam.png\")\n",
    "\n",
    "    return {\"image\": image_path,\n",
    "            \"stage1_prediction\": \"DR\",\n",
    "            \"stage1_conf\": conf1,\n",
    "            \"stage1_heatmap\": overlay1,\n",
    "            \"stage2_prediction\": f\"DR Severity {severity}\",\n",
    "            \"stage2_conf\": conf2,\n",
    "            \"stage2_heatmap\": overlay2,\n",
    "            \"final_prediction\": f\"DR Severity {severity}\"}\n",
    "\n",
    "# -------------------- Example Usage --------------------\n",
    "image_paths = [\n",
    "    \"/kaggle/input/dr-dataset/BPEF_WORKSHOPS/EyePACS/test/0/10701_left.jpeg\",\n",
    "]\n",
    "\n",
    "for path in image_paths:\n",
    "    result = predict_with_gradcam(path)\n",
    "    print(result[\"final_prediction\"])\n",
    "\n",
    "    # Show Stage 1 heatmap\n",
    "    plt.imshow(result[\"stage1_heatmap\"])\n",
    "    plt.title(\"Stage 1 Grad-CAM\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # If DR → Show Stage 2 heatmap\n",
    "    if \"stage2_heatmap\" in result:\n",
    "        plt.imshow(result[\"stage2_heatmap\"])\n",
    "        plt.title(\"Stage 2 Grad-CAM\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8220320,
     "sourceId": 12987238,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 451408,
     "modelInstanceId": 434554,
     "sourceId": 582083,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 451412,
     "modelInstanceId": 434558,
     "sourceId": 582087,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
